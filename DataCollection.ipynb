{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This notebook contains collection of data from cragslist. We collected 923 records. We have used MongoDB for data storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import urllib2\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client['craigslist']\n",
    "freebies_coll = db['freebies']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base urls needs to be Processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "page_url = 'https://chicago.craigslist.org'\n",
    "list_url = page_url+'/search/zip?s='"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch Items from Cragslist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_title(soup_html):\n",
    "    \"\"\"Gets Title from html soup object\"\"\"\n",
    "    return soup_html.title.string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_desc(soup_html):\n",
    "    \"\"\"Gets Description from soup object\"\"\"\n",
    "    desc = ''\n",
    "    for s in soup_html.find(id=\"postingbody\").contents:\n",
    "        desc = desc + ' ' + unicode(s)\n",
    "    return desc\n",
    "    #return unicode(soup_html.find(id=\"postingbody\").string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_location(soup_html):\n",
    "    \"\"\"Gets Location from soup object\"\"\"\n",
    "    location = {}\n",
    "    if(soup_html.find(id=\"map\")):\n",
    "        location['latitude'] = soup_html.find(id=\"map\")['data-latitude']\n",
    "        location['longitude'] = soup_html.find(id=\"map\")['data-longitude']\n",
    "        location['accuracy'] = soup_html.find(id=\"map\")['data-accuracy']\n",
    "    return location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_posted_date(soup_html):\n",
    "    \"\"\"Gets when the Item was posted\"\"\"\n",
    "    return soup_html.time['datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_contents(url):\n",
    "    \"\"\"Get Item content like title, description from given url\"\"\"\n",
    "    response = urllib2.urlopen(url)\n",
    "    html = response.read()\n",
    "    soup_html = BeautifulSoup(html, 'html.parser')\n",
    "    content = {}\n",
    "    content['title'] = get_title(soup_html)\n",
    "    content['desc'] = get_desc(soup_html)\n",
    "    if get_location(soup_html):\n",
    "        content['location'] = get_location(soup_html)\n",
    "    content['posteddate'] = get_posted_date(soup_html)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ids(url):\n",
    "    \"\"\"Given the list url gets All the Item Ids from it which will be later used to obtain Item Info\"\"\"\n",
    "    ids = []\n",
    "    response = urllib2.urlopen(url)\n",
    "    html = response.read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    soup_p = (soup.find_all('p'))\n",
    "    for p in soup_p:\n",
    "        ids.append((p.get('data-pid'),p.a['href']))\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def insert_to_db(key,data):\n",
    "    freebies_coll.replace_one(key,content,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cralled 100 page\n",
      "cralled 200 page\n",
      "cralled 300 page\n",
      "cralled 400 page\n",
      "cralled 500 page\n",
      "cralled 600 page\n",
      "cralled 700 page\n",
      "cralled 800 page\n",
      "cralled 900 page\n",
      "cralled 1000 page\n",
      "cralled 1100 page\n"
     ]
    }
   ],
   "source": [
    "for page in range(0,1100,100):\n",
    "    for _id,url in get_ids(list_url+str(page)):\n",
    "        content = get_contents(page_url+str(url))\n",
    "        #content['_id'] = _id\n",
    "        insert_to_db({'_id':_id},content)\n",
    "    print 'cralled %d page'%(page+100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
